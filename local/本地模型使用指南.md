# 本地Qwen模型使用指南

## 🎯 配置概览

您的博客系统已配置为使用本地Qwen模型，提供以下特性：

- **模型名称**：Qwen3-4B-I-chat
- **模型路径**：`D:\code\llm\wyh\llm\Qwen3\Qwen3-4B-I-chat`
- **容器内路径**：`/models/Qwen3-4B-I-chat`
- **API端口**：8000
- **最大上下文长度**：4096 tokens
- **GPU内存使用率**：85%

## 📋 前置要求

### 1. 模型文件准备

确保以下文件存在于模型目录中：

```
D:\code\llm\wyh\llm\Qwen3\Qwen3-4B-I-chat\
├── config.json                 # 模型配置文件
├── tokenizer.json             # 分词器文件
├── pytorch_model.bin          # 模型权重文件
├── tokenizer_config.json      # 分词器配置
└── special_tokens_map.json    # 特殊标记映射
```

### 2. 硬件要求

- **内存**：至少8GB RAM
- **磁盘空间**：至少10GB可用空间
- **GPU**：推荐NVIDIA GPU（可选）

## 🚀 启动步骤

### 1. 验证模型文件

```bash
# 检查模型目录
dir "D:\code\llm\wyh\llm\Qwen3\Qwen3-4B-I-chat"

# 验证配置文件
python -c "
import json
with open(r'D:\code\llm\wyh\llm\Qwen3\Qwen3-4B-I-chat\config.json', 'r') as f:
    config = json.load(f)
    print(f'模型类型: {config.get(\"model_type\", \"未知\")}')
    print(f'词汇表大小: {config.get(\"vocab_size\", \"未知\")}')
"
```

### 2. 启动vLLM服务

```bash
# 进入项目目录
cd /path/to/your/blog-project

# 启动vLLM服务（会自动挂载本地模型）
docker-compose up -d vllm
```

### 3. 验证服务状态

```bash
# 检查vLLM服务状态
curl http://localhost:8000/health

# 获取模型列表
curl http://localhost:8000/v1/models

# 测试聊天功能
curl -X POST http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "Qwen3-4B-I-chat",
    "messages": [{"role": "user", "content": "你好，请介绍一下你自己"}],
    "max_tokens": 200
  }'
```

## ⚙️ 配置详解

### Docker Compose配置

```yaml
vllm:
  image: vllm/vllm-openai:latest
  container_name: vllm
  ports:
    - "8000:8000"
  environment:
    - MODEL_NAME=Qwen3-4B-I-chat
  volumes:
    - D:\code\llm\wyh\llm\Qwen3\Qwen3-4B-I-chat:/models/Qwen3-4B-I-chat:ro
    - ./config:/config:ro
  command: >
    --model /models/Qwen3-4B-I-chat
    --host 0.0.0.0
    --port 8000
    --max-model-len 4096
    --gpu-memory-utilization 0.8
    --trust-remote-code
```

### 应用配置

在`application.yml`中配置：

```yaml
ai:
  vllm:
    url: http://localhost:8000/v1/chat/completions
    model: Qwen3-4B-I-chat
    timeout: 180000
    max-tokens: 2048
    temperature: 0.7
    top-p: 0.9
```

## 🔧 故障排查

### 常见问题

#### 1. 模型文件不存在

**现象**：vLLM启动失败，提示找不到模型文件

**解决**：
```bash
# 检查模型路径
dir "D:\code\llm\wyh\llm\Qwen3\Qwen3-4B-I-chat"

# 验证必需文件
# - config.json
# - tokenizer.json 或 tokenizer.model
# - pytorch_model.bin 或 model.safetensors
```

#### 2. 权限不足

**现象**：Docker容器无法访问模型目录

**解决**：
```bash
# 检查目录权限
icacls "D:\code\llm\wyh\llm\Qwen3\Qwen3-4B-I-chat"

# 给Docker用户添加读取权限
# 或将模型复制到用户有权限的目录
```

#### 3. GPU内存不足

**现象**：CUDA out of memory错误

**解决**：
```bash
# 降低GPU内存使用率
# 修改docker-compose.yml中的gpu-memory-utilization为0.7或更低
```

#### 4. 模型加载失败

**现象**：模型配置错误或文件损坏

**解决**：
```bash
# 验证模型配置
python -c "
import json
try:
    with open(r'D:\code\llm\wyh\llm\Qwen3\Qwen3-4B-I-chat\config.json', 'r') as f:
        config = json.load(f)
    print('模型配置有效')
except Exception as e:
    print(f'模型配置错误: {e}')
"
```

## 📊 性能监控

### 查看GPU使用情况

```bash
# NVIDIA GPU监控
nvidia-smi

# Docker容器资源使用
docker stats vllm
```

### 查看vLLM日志

```bash
# Docker日志
docker-compose logs -f vllm

# 检查模型加载日志
# 查找 "Loading model" 或 "Model loaded" 关键词
```

### API响应测试

```bash
# 健康检查
curl -f http://localhost:8000/health

# 模型信息
curl http://localhost:8000/v1/models

# 简单对话测试
curl -X POST http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{"model": "Qwen3-4B-I-chat", "messages": [{"role": "user", "content": "测试"}], "max_tokens": 50}'
```

## 🔄 更新模型

### 如需更换模型

1. **停止当前服务**：
   ```bash
   docker-compose stop vllm
   ```

2. **更新模型文件**：
   - 将新模型文件放入指定目录
   - 更新`docker-compose.yml`中的模型路径
   - 更新`application.yml`中的模型名称

3. **重启服务**：
   ```bash
   docker-compose up -d vllm
   ```

### 模型兼容性

确保新模型格式与vLLM兼容：
- 支持的格式：Hugging Face格式
- 必需文件：config.json, tokenizer相关文件, 模型权重文件

## 📚 参考资料

- [vLLM官方文档](https://vllm.readthedocs.io/)
- [Qwen模型介绍](https://qwenlm.github.io/)
- [Docker卷挂载指南](https://docs.docker.com/storage/volumes/)

## 🤝 支持

如果遇到问题，请：

1. 检查模型文件完整性
2. 确认Docker配置正确
3. 查看容器日志
4. 验证网络连接

如需帮助，请联系开发团队或提交Issue。

---

**享受您的本地AI体验！** 🎉

