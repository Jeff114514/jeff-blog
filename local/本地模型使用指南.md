# æœ¬åœ°Qwenæ¨¡å‹ä½¿ç”¨æŒ‡å—

## ğŸ¯ é…ç½®æ¦‚è§ˆ

æ‚¨çš„åšå®¢ç³»ç»Ÿå·²é…ç½®ä¸ºä½¿ç”¨æœ¬åœ°Qwenæ¨¡å‹ï¼Œæä¾›ä»¥ä¸‹ç‰¹æ€§ï¼š

- **æ¨¡å‹åç§°**ï¼šQwen3-4B-I-chat
- **æ¨¡å‹è·¯å¾„**ï¼š`D:\code\llm\wyh\llm\Qwen3\Qwen3-4B-I-chat`
- **å®¹å™¨å†…è·¯å¾„**ï¼š`/models/Qwen3-4B-I-chat`
- **APIç«¯å£**ï¼š8000
- **æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦**ï¼š4096 tokens
- **GPUå†…å­˜ä½¿ç”¨ç‡**ï¼š85%

## ğŸ“‹ å‰ç½®è¦æ±‚

### 1. æ¨¡å‹æ–‡ä»¶å‡†å¤‡

ç¡®ä¿ä»¥ä¸‹æ–‡ä»¶å­˜åœ¨äºæ¨¡å‹ç›®å½•ä¸­ï¼š

```
D:\code\llm\wyh\llm\Qwen3\Qwen3-4B-I-chat\
â”œâ”€â”€ config.json                 # æ¨¡å‹é…ç½®æ–‡ä»¶
â”œâ”€â”€ tokenizer.json             # åˆ†è¯å™¨æ–‡ä»¶
â”œâ”€â”€ pytorch_model.bin          # æ¨¡å‹æƒé‡æ–‡ä»¶
â”œâ”€â”€ tokenizer_config.json      # åˆ†è¯å™¨é…ç½®
â””â”€â”€ special_tokens_map.json    # ç‰¹æ®Šæ ‡è®°æ˜ å°„
```

### 2. ç¡¬ä»¶è¦æ±‚

- **å†…å­˜**ï¼šè‡³å°‘8GB RAM
- **ç£ç›˜ç©ºé—´**ï¼šè‡³å°‘10GBå¯ç”¨ç©ºé—´
- **GPU**ï¼šæ¨èNVIDIA GPUï¼ˆå¯é€‰ï¼‰

## ğŸš€ å¯åŠ¨æ­¥éª¤

### 1. éªŒè¯æ¨¡å‹æ–‡ä»¶

```bash
# æ£€æŸ¥æ¨¡å‹ç›®å½•
dir "D:\code\llm\wyh\llm\Qwen3\Qwen3-4B-I-chat"

# éªŒè¯é…ç½®æ–‡ä»¶
python -c "
import json
with open(r'D:\code\llm\wyh\llm\Qwen3\Qwen3-4B-I-chat\config.json', 'r') as f:
    config = json.load(f)
    print(f'æ¨¡å‹ç±»å‹: {config.get(\"model_type\", \"æœªçŸ¥\")}')
    print(f'è¯æ±‡è¡¨å¤§å°: {config.get(\"vocab_size\", \"æœªçŸ¥\")}')
"
```

### 2. å¯åŠ¨vLLMæœåŠ¡

```bash
# è¿›å…¥é¡¹ç›®ç›®å½•
cd /path/to/your/blog-project

# å¯åŠ¨vLLMæœåŠ¡ï¼ˆä¼šè‡ªåŠ¨æŒ‚è½½æœ¬åœ°æ¨¡å‹ï¼‰
docker-compose up -d vllm
```

### 3. éªŒè¯æœåŠ¡çŠ¶æ€

```bash
# æ£€æŸ¥vLLMæœåŠ¡çŠ¶æ€
curl http://localhost:8000/health

# è·å–æ¨¡å‹åˆ—è¡¨
curl http://localhost:8000/v1/models

# æµ‹è¯•èŠå¤©åŠŸèƒ½
curl -X POST http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "Qwen3-4B-I-chat",
    "messages": [{"role": "user", "content": "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±"}],
    "max_tokens": 200
  }'
```

## âš™ï¸ é…ç½®è¯¦è§£

### Docker Composeé…ç½®

```yaml
vllm:
  image: vllm/vllm-openai:latest
  container_name: vllm
  ports:
    - "8000:8000"
  environment:
    - MODEL_NAME=Qwen3-4B-I-chat
  volumes:
    - D:\code\llm\wyh\llm\Qwen3\Qwen3-4B-I-chat:/models/Qwen3-4B-I-chat:ro
    - ./config:/config:ro
  command: >
    --model /models/Qwen3-4B-I-chat
    --host 0.0.0.0
    --port 8000
    --max-model-len 4096
    --gpu-memory-utilization 0.8
    --trust-remote-code
```

### åº”ç”¨é…ç½®

åœ¨`application.yml`ä¸­é…ç½®ï¼š

```yaml
ai:
  vllm:
    url: http://localhost:8000/v1/chat/completions
    model: Qwen3-4B-I-chat
    timeout: 180000
    max-tokens: 2048
    temperature: 0.7
    top-p: 0.9
```

## ğŸ”§ æ•…éšœæ’æŸ¥

### å¸¸è§é—®é¢˜

#### 1. æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨

**ç°è±¡**ï¼švLLMå¯åŠ¨å¤±è´¥ï¼Œæç¤ºæ‰¾ä¸åˆ°æ¨¡å‹æ–‡ä»¶

**è§£å†³**ï¼š
```bash
# æ£€æŸ¥æ¨¡å‹è·¯å¾„
dir "D:\code\llm\wyh\llm\Qwen3\Qwen3-4B-I-chat"

# éªŒè¯å¿…éœ€æ–‡ä»¶
# - config.json
# - tokenizer.json æˆ– tokenizer.model
# - pytorch_model.bin æˆ– model.safetensors
```

#### 2. æƒé™ä¸è¶³

**ç°è±¡**ï¼šDockerå®¹å™¨æ— æ³•è®¿é—®æ¨¡å‹ç›®å½•

**è§£å†³**ï¼š
```bash
# æ£€æŸ¥ç›®å½•æƒé™
icacls "D:\code\llm\wyh\llm\Qwen3\Qwen3-4B-I-chat"

# ç»™Dockerç”¨æˆ·æ·»åŠ è¯»å–æƒé™
# æˆ–å°†æ¨¡å‹å¤åˆ¶åˆ°ç”¨æˆ·æœ‰æƒé™çš„ç›®å½•
```

#### 3. GPUå†…å­˜ä¸è¶³

**ç°è±¡**ï¼šCUDA out of memoryé”™è¯¯

**è§£å†³**ï¼š
```bash
# é™ä½GPUå†…å­˜ä½¿ç”¨ç‡
# ä¿®æ”¹docker-compose.ymlä¸­çš„gpu-memory-utilizationä¸º0.7æˆ–æ›´ä½
```

#### 4. æ¨¡å‹åŠ è½½å¤±è´¥

**ç°è±¡**ï¼šæ¨¡å‹é…ç½®é”™è¯¯æˆ–æ–‡ä»¶æŸå

**è§£å†³**ï¼š
```bash
# éªŒè¯æ¨¡å‹é…ç½®
python -c "
import json
try:
    with open(r'D:\code\llm\wyh\llm\Qwen3\Qwen3-4B-I-chat\config.json', 'r') as f:
        config = json.load(f)
    print('æ¨¡å‹é…ç½®æœ‰æ•ˆ')
except Exception as e:
    print(f'æ¨¡å‹é…ç½®é”™è¯¯: {e}')
"
```

## ğŸ“Š æ€§èƒ½ç›‘æ§

### æŸ¥çœ‹GPUä½¿ç”¨æƒ…å†µ

```bash
# NVIDIA GPUç›‘æ§
nvidia-smi

# Dockerå®¹å™¨èµ„æºä½¿ç”¨
docker stats vllm
```

### æŸ¥çœ‹vLLMæ—¥å¿—

```bash
# Dockeræ—¥å¿—
docker-compose logs -f vllm

# æ£€æŸ¥æ¨¡å‹åŠ è½½æ—¥å¿—
# æŸ¥æ‰¾ "Loading model" æˆ– "Model loaded" å…³é”®è¯
```

### APIå“åº”æµ‹è¯•

```bash
# å¥åº·æ£€æŸ¥
curl -f http://localhost:8000/health

# æ¨¡å‹ä¿¡æ¯
curl http://localhost:8000/v1/models

# ç®€å•å¯¹è¯æµ‹è¯•
curl -X POST http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{"model": "Qwen3-4B-I-chat", "messages": [{"role": "user", "content": "æµ‹è¯•"}], "max_tokens": 50}'
```

## ğŸ”„ æ›´æ–°æ¨¡å‹

### å¦‚éœ€æ›´æ¢æ¨¡å‹

1. **åœæ­¢å½“å‰æœåŠ¡**ï¼š
   ```bash
   docker-compose stop vllm
   ```

2. **æ›´æ–°æ¨¡å‹æ–‡ä»¶**ï¼š
   - å°†æ–°æ¨¡å‹æ–‡ä»¶æ”¾å…¥æŒ‡å®šç›®å½•
   - æ›´æ–°`docker-compose.yml`ä¸­çš„æ¨¡å‹è·¯å¾„
   - æ›´æ–°`application.yml`ä¸­çš„æ¨¡å‹åç§°

3. **é‡å¯æœåŠ¡**ï¼š
   ```bash
   docker-compose up -d vllm
   ```

### æ¨¡å‹å…¼å®¹æ€§

ç¡®ä¿æ–°æ¨¡å‹æ ¼å¼ä¸vLLMå…¼å®¹ï¼š
- æ”¯æŒçš„æ ¼å¼ï¼šHugging Faceæ ¼å¼
- å¿…éœ€æ–‡ä»¶ï¼šconfig.json, tokenizerç›¸å…³æ–‡ä»¶, æ¨¡å‹æƒé‡æ–‡ä»¶

## ğŸ“š å‚è€ƒèµ„æ–™

- [vLLMå®˜æ–¹æ–‡æ¡£](https://vllm.readthedocs.io/)
- [Qwenæ¨¡å‹ä»‹ç»](https://qwenlm.github.io/)
- [Dockerå·æŒ‚è½½æŒ‡å—](https://docs.docker.com/storage/volumes/)

## ğŸ¤ æ”¯æŒ

å¦‚æœé‡åˆ°é—®é¢˜ï¼Œè¯·ï¼š

1. æ£€æŸ¥æ¨¡å‹æ–‡ä»¶å®Œæ•´æ€§
2. ç¡®è®¤Dockeré…ç½®æ­£ç¡®
3. æŸ¥çœ‹å®¹å™¨æ—¥å¿—
4. éªŒè¯ç½‘ç»œè¿æ¥

å¦‚éœ€å¸®åŠ©ï¼Œè¯·è”ç³»å¼€å‘å›¢é˜Ÿæˆ–æäº¤Issueã€‚

---

**äº«å—æ‚¨çš„æœ¬åœ°AIä½“éªŒï¼** ğŸ‰

