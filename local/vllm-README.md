# vLLM AIæœåŠ¡éƒ¨ç½²æŒ‡å—

## æ¦‚è¿°

vLLMæ˜¯ä¸€ä¸ªé«˜æ€§èƒ½çš„å¤§è¯­è¨€æ¨¡å‹æ¨ç†æ¡†æ¶ï¼Œç”¨äºæ›¿ä»£åŸæœ‰çš„OllamaæœåŠ¡ã€‚vLLMæä¾›äº†ä¸OpenAI APIå…¼å®¹çš„æ¥å£ï¼Œå…·æœ‰æ›´é«˜çš„æ€§èƒ½å’Œæ›´å¥½çš„å†…å­˜ç®¡ç†ã€‚

## ä¸»è¦ä¼˜åŠ¿

- ğŸš€ **é«˜æ€§èƒ½**ï¼šæ¯”ä¼ ç»Ÿæ¨ç†æ¡†æ¶å¿«æ•°å€
- ğŸ’¾ **å†…å­˜ä¼˜åŒ–**ï¼šæ”¯æŒæ›´å¤§çš„æ¨¡å‹å’Œæ›´é«˜çš„ååé‡
- ğŸ”§ **æ˜“é›†æˆ**ï¼šæä¾›OpenAIå…¼å®¹çš„APIæ¥å£
- âš¡ **æ‰¹å¤„ç†**ï¼šæ”¯æŒåŠ¨æ€æ‰¹å¤„ç†å’Œè¿ç»­æ‰¹å¤„ç†
- ğŸ› ï¸ **æ˜“æ‰©å±•**ï¼šæ”¯æŒæ¨¡å‹å¹¶è¡Œå’Œå¼ é‡å¹¶è¡Œ

## ç³»ç»Ÿè¦æ±‚

### ç¡¬ä»¶è¦æ±‚
- **GPU**ï¼šæ¨èNVIDIA GPUï¼ˆRTX 30ç³»åˆ—æˆ–æ›´é«˜ï¼‰
- **å†…å­˜**ï¼šè‡³å°‘8GB RAM
- **ç¡¬ç›˜**ï¼šè‡³å°‘50GBå¯ç”¨ç©ºé—´ï¼ˆç”¨äºæ¨¡å‹å­˜å‚¨ï¼‰

### è½¯ä»¶è¦æ±‚
- Python 3.8+
- CUDA 11.8+
- PyTorch 2.0+
- ç½‘ç»œè¿æ¥ï¼ˆç”¨äºä¸‹è½½æ¨¡å‹ï¼‰

## å®‰è£…æ–¹å¼

### æ–¹å¼ä¸€ï¼šDockeréƒ¨ç½²ï¼ˆæ¨èï¼‰

vLLMæœåŠ¡å·²åœ¨é¡¹ç›®çš„`docker-compose.yml`ä¸­é…ç½®ï¼Œä¼šè‡ªåŠ¨å¯åŠ¨ã€‚

```bash
cd local
docker-compose up -d vllm
```

### æ–¹å¼äºŒï¼šæ‰‹åŠ¨å®‰è£…

```bash
# å®‰è£…vLLM
pip install vllm

# æˆ–è€…ä»æºç å®‰è£…
git clone https://github.com/vllm-project/vllm.git
cd vllm
pip install -e .
```

## æ¨¡å‹å‡†å¤‡

### æœ¬åœ°æ¨¡å‹é…ç½®

å½“å‰é¡¹ç›®å·²é…ç½®ä¸ºä½¿ç”¨æœ¬åœ°Qwenæ¨¡å‹ï¼š
- **æ¨¡å‹è·¯å¾„**ï¼š`D:\code\llm\wyh\llm\Qwen3\Qwen3-4B-I-chat`
- **å®¹å™¨å†…è·¯å¾„**ï¼š`/models/Qwen3-4B-I-chat`
- **æ¨¡å‹ç±»å‹**ï¼šQwen3-4B-I-chat

### ä¸‹è½½æ¨¡å‹

**ä½¿ç”¨Dockerï¼ˆæ¨èï¼‰**ï¼š

æ¨¡å‹ä¼šè‡ªåŠ¨ä¸‹è½½åˆ°å®¹å™¨å†…çš„`/models`ç›®å½•ã€‚

**æ‰‹åŠ¨ä¸‹è½½**ï¼š

**æœ¬åœ°æ¨¡å‹ä½¿ç”¨**ï¼š
é¡¹ç›®å·²é…ç½®ä¸ºä½¿ç”¨æŒ‡å®šçš„æœ¬åœ°æ¨¡å‹è·¯å¾„ã€‚ç¡®ä¿æ¨¡å‹æ–‡ä»¶å®Œæ•´ï¼š

**å¿…éœ€çš„æ–‡ä»¶**ï¼š
- `config.json` - æ¨¡å‹é…ç½®æ–‡ä»¶
- `tokenizer.json` æˆ– `tokenizer.model` - åˆ†è¯å™¨æ–‡ä»¶
- `pytorch_model.bin` æˆ– `model.safetensors` - æ¨¡å‹æƒé‡æ–‡ä»¶
- å…¶ä»–å¿…è¦çš„æ¨¡å‹ç»„ä»¶æ–‡ä»¶

**æ¨¡å‹è·¯å¾„**ï¼š`D:\code\llm\wyh\llm\Qwen3\Qwen3-4B-I-chat`

**éªŒè¯æ¨¡å‹**ï¼š
```bash
# æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨
ls -la "D:\code\llm\wyh\llm\Qwen3\Qwen3-4B-I-chat"

# éªŒè¯æ¨¡å‹å®Œæ•´æ€§
python -c "
import json
import os

model_path = r'D:\code\llm\wyh\llm\Qwen3\Qwen3-4B-I-chat'
config_file = os.path.join(model_path, 'config.json')

if os.path.exists(config_file):
    with open(config_file, 'r') as f:
        config = json.load(f)
    print(f'æ¨¡å‹é…ç½®åŠ è½½æˆåŠŸ: {config.get(\"model_type\", \"æœªçŸ¥\")}')
    print(f'è¯æ±‡è¡¨å¤§å°: {config.get(\"vocab_size\", \"æœªçŸ¥\")}')
else:
    print('æ¨¡å‹é…ç½®æ–‡ä»¶ä¸å­˜åœ¨')
"
```

## å¯åŠ¨æœåŠ¡

### Dockeræ–¹å¼

æœåŠ¡å·²åœ¨`docker-compose.yml`ä¸­é…ç½®ä¸ºä½¿ç”¨æœ¬åœ°æ¨¡å‹ï¼š

```yaml
vllm:
  image: vllm/vllm-openai:latest
  container_name: vllm
  ports:
    - "8000:8000"
  environment:
    - MODEL_NAME=Qwen3-4B-I-chat
  volumes:
    - D:\code\llm\wyh\llm\Qwen3\Qwen3-4B-I-chat:/models/Qwen3-4B-I-chat:ro
    - ./config:/config:ro
  command: >
    --model /models/Qwen3-4B-I-chat
    --host 0.0.0.0
    --port 8000
    --config /config/vllm_config.json
```

å¯åŠ¨å‘½ä»¤ï¼š
```bash
docker-compose up -d vllm
```

### æ‰‹åŠ¨æ–¹å¼

```bash
# å¯åŠ¨vLLMæœåŠ¡
python -m vllm.entrypoints.openai.api_server \
  --model meta-llama/Llama-2-7b-chat-hf \
  --host 0.0.0.0 \
  --port 8000 \
  --max-model-len 2048 \
  --gpu-memory-utilization 0.9 \
  --tensor-parallel-size 1
```

## APIæ¥å£

vLLMæä¾›ä¸OpenAIå…¼å®¹çš„APIæ¥å£ã€‚

### å¥åº·æ£€æŸ¥

```bash
curl http://localhost:8000/health
```

### è·å–æ¨¡å‹åˆ—è¡¨

```bash
curl http://localhost:8000/v1/models
```

### èŠå¤©æ¥å£

```bash
curl -X POST http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "meta-llama/Llama-2-7b-chat-hf",
    "messages": [
      {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„åŠ©æ‰‹"},
      {"role": "user", "content": "ä½ å¥½"}
    ],
    "max_tokens": 100,
    "temperature": 0.7
  }'
```

## é…ç½®è¯´æ˜

### åç«¯é…ç½®

åœ¨`application.yml`ä¸­é…ç½®ï¼š

```yaml
ai:
  vllm:
    url: http://localhost:8000/v1/chat/completions
    model: meta-llama/Llama-2-7b-chat-hf
    api-key: your-api-key-here  # å¯é€‰
    timeout: 180000
    max-tokens: 1024
    temperature: 0.7
    top-p: 0.9
```

### ç¯å¢ƒå˜é‡

| å˜é‡å | é»˜è®¤å€¼ | è¯´æ˜ |
|--------|--------|------|
| MODEL_NAME | meta-llama/Llama-2-7b-chat-hf | æ¨¡å‹åç§° |
| MAX_MODEL_LEN | 2048 | æœ€å¤§åºåˆ—é•¿åº¦ |
| GPU_MEMORY_UTILIZATION | 0.9 | GPUå†…å­˜ä½¿ç”¨ç‡ |
| TENSOR_PARALLEL_SIZE | 1 | å¼ é‡å¹¶è¡Œå¤§å° |

## æ€§èƒ½è°ƒä¼˜

### GPUä¼˜åŒ–

```bash
# æ£€æŸ¥GPUçŠ¶æ€
nvidia-smi

# ç›‘æ§GPUä½¿ç”¨ç‡
watch -n 1 nvidia-smi
```

### å†…å­˜ä¼˜åŒ–

- è°ƒæ•´`--gpu-memory-utilization`å‚æ•°
- ä½¿ç”¨`--max-model-len`é™åˆ¶åºåˆ—é•¿åº¦
- å¯ç”¨`--tensor-parallel-size`è¿›è¡Œå¼ é‡å¹¶è¡Œ

### æ‰¹å¤„ç†ä¼˜åŒ–

vLLMè‡ªåŠ¨å¤„ç†æ‰¹å¤„ç†ï¼Œé€šå¸¸ä¸éœ€è¦æ‰‹åŠ¨é…ç½®ã€‚å¯ä»¥é€šè¿‡ä»¥ä¸‹å‚æ•°è°ƒæ•´ï¼š

```bash
--max-num-seqs 256        # æœ€å¤§åºåˆ—æ•°
--max-num-batched-tokens  # æœ€å¤§æ‰¹å¤„ç†tokenæ•°
```

## æ•…éšœæ’æŸ¥

### å¸¸è§é—®é¢˜

#### 1. GPUå†…å­˜ä¸è¶³

**ç°è±¡**ï¼šCUDA out of memoryé”™è¯¯

**è§£å†³æ–¹æ¡ˆ**ï¼š
- é™ä½`--gpu-memory-utilization`å€¼
- å‡å°‘`--max-model-len`é•¿åº¦
- ä½¿ç”¨æ›´å°çš„æ¨¡å‹

#### 2. æ¨¡å‹ä¸‹è½½å¤±è´¥

**ç°è±¡**ï¼šæ¨¡å‹ä¸‹è½½è¶…æ—¶æˆ–å¤±è´¥

**è§£å†³æ–¹æ¡ˆ**ï¼š
- æ£€æŸ¥ç½‘ç»œè¿æ¥
- ä½¿ç”¨å›½å†…é•œåƒæº
- æ‰‹åŠ¨ä¸‹è½½æ¨¡å‹æ–‡ä»¶

#### 3. æœåŠ¡å¯åŠ¨å¤±è´¥

**ç°è±¡**ï¼šç«¯å£è¢«å ç”¨æˆ–CUDAé”™è¯¯

**è§£å†³æ–¹æ¡ˆ**ï¼š
```bash
# æ£€æŸ¥ç«¯å£
netstat -tulpn | grep 8000

# é‡Šæ”¾ç«¯å£
kill -9 <PID>

# æ£€æŸ¥CUDAç‰ˆæœ¬
nvidia-smi
nvcc --version
```

### æ—¥å¿—æŸ¥çœ‹

```bash
# Dockeræ—¥å¿—
docker-compose logs -f vllm

# æ‰‹åŠ¨å®‰è£…æ—¥å¿—
tail -f /var/log/vllm.log
```

## ç›‘æ§å’Œç»´æŠ¤

### æ€§èƒ½ç›‘æ§

```bash
# å®æ—¶æ€§èƒ½ç›‘æ§
docker stats vllm

# GPUç›‘æ§
nvidia-smi -l 1

# æœåŠ¡å¥åº·æ£€æŸ¥
curl -f http://localhost:8000/health || echo "æœåŠ¡å¼‚å¸¸"
```

### å®šæœŸç»´æŠ¤

1. **æ¨¡å‹æ›´æ–°**ï¼šå®šæœŸæ›´æ–°åˆ°æœ€æ–°ç‰ˆæœ¬çš„æ¨¡å‹
2. **ä¾èµ–æ›´æ–°**ï¼šä¿æŒvLLMå’Œç›¸å…³ä¾èµ–ä¸ºæœ€æ–°ç‰ˆæœ¬
3. **æ—¥å¿—æ¸…ç†**ï¼šå®šæœŸæ¸…ç†æ—§æ—¥å¿—æ–‡ä»¶
4. **æ€§èƒ½ä¼˜åŒ–**ï¼šæ ¹æ®ä½¿ç”¨æƒ…å†µè°ƒæ•´å‚æ•°

## æ‰©å±•é…ç½®

### å¤šæ¨¡å‹æ”¯æŒ

vLLMæ”¯æŒåŒæ—¶åŠ è½½å¤šä¸ªæ¨¡å‹ï¼š

```bash
python -m vllm.entrypoints.openai.api_server \
  --model meta-llama/Llama-2-7b-chat-hf \
  --model microsoft/DialoGPT-medium \
  --host 0.0.0.0 \
  --port 8000
```

### é«˜å¯ç”¨éƒ¨ç½²

```yaml
# docker-composeé«˜å¯ç”¨é…ç½®
vllm:
  image: vllm/vllm-openai:latest
  deploy:
    replicas: 2
  # ... å…¶ä»–é…ç½®
```

## å‚è€ƒèµ„æ–™

- [vLLMå®˜æ–¹æ–‡æ¡£](https://vllm.readthedocs.io/)
- [vLLM GitHub](https://github.com/vllm-project/vllm)
- [Hugging Faceæ¨¡å‹](https://huggingface.co/models)
- [OpenAI APIæ–‡æ¡£](https://platform.openai.com/docs/api-reference)

## æ›´æ–°æ—¥å¿—

### v1.0.0 (2025-10-10)
- åˆå§‹ç‰ˆæœ¬ï¼Œæ”¯æŒLlama-2æ¨¡å‹
- Dockerå®¹å™¨åŒ–éƒ¨ç½²
- OpenAIå…¼å®¹APIæ¥å£
- é«˜æ€§èƒ½æ¨ç†ä¼˜åŒ–

---

**æ³¨æ„**ï¼šä½¿ç”¨vLLMéœ€è¦ç¡®ä¿æ¨¡å‹æ–‡ä»¶çš„åˆæ³•æ€§å’Œç‰ˆæƒåˆè§„ã€‚è¯·éµå®ˆç›¸å…³å¼€æºåè®®å’Œä½¿ç”¨æ¡æ¬¾ã€‚

